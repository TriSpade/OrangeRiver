Useful Information

Bring fully-charged laptop with Respondus lockdown browser for exam
He said in class to be able to understand the code for Transportation sort, Merge sort, and Quicksort, and to be able to tell the value(s) of the array(s) being sorted at any step of the code.



Test Format (less than 25 questions)[FROM LAST YEAR]

 

40% questions from review notes

   - disregard Distributed Shared Memory

40% code reading/correction

   - no syntax correction

   - nothing with array initialization

   - mostly logical corrections with how parallelization works (send, receive, etc.)

   - "what is the final value of this code"

   - doing sequential code only gets half of the points

20% (pseudo) code writing



Test 1

 

Give an example of demand for computation speed
Forecasting trajectory of hurricanes

 

In a sentence, define speedup
A ratio of how much faster a program can run once some computing resources are added

 

In a sentence, define efficiency
The ratio of performance improvement per individual unit of computing resource

 

Describe two limiting factors in achieving maximum parallelism
Not all code is parallelizable
Communication overhead
 

Describe linear speedup? What types of applications will exhibit linear speedup?
When the observed speedup ratio is equal to the amount of parallelization applied, ie, S(P)=P

Don’t know what types of applications will exhibit linear speedup, please advise1

Extremely pleasantly parallel programs will have very close to linear speedup

What is superlinear speedup? What can cause superlinear speedup?
When the observed speedup ratio is higher than the amount of parallelization applied, ie, S(P)>P

Poor sequential reference implementation
Memory caching
I/O Blocking
 

In a sentence, define Amdahl’s Law. Give the equation for Amdahl’s Law.
Formula for theoretical max speedup

S(p) = speedup

P = number of processors

f = fraction of code that is not parallelizable

 

Suppose that I get a speedup of 8 when I run my application on 10 processors. According to Amdahl's Law, what portion is serial? What is the speedup on 20 processors? What is the efficiency? What is the best speedup that I could hope for?
S(p) = 8 = 10/((10-1)(f)+1) = 10/(9f+1) => 72f + 8 = 10 => f = 1/36

S(20) = 20 / ((19*1/36)+1) = 13.090909

Efficiency = 1/((8-1)(1/36)+1)*100% = 1/(43/36)*100% = 83.72%

Best speedup = 1/(1/36) = 36

 

Suppose that 4% of my application is serial. What is my predicted speedup according to Amdahl’s Law on 5 processors?
S(5) = 5/(4*0.04+1) = 4.31




For each of the following type of parallel computers, briefly describe its architecture, provide its schematic, and name a programming model



SIMD (ALLEGEDLY WON’T BE ON TEST)
 

Shared memory
One process, multiple threads
All threads have read/write access to same memory
Threads, OpenMP



Distributed memory – message passing
Processes handle their own memory, data passes between processes via messages
Scales well
Commodity parts
Expandable
Heterogenous



Distributed shared memory
Message passing
Same as shared memory, but on multiple nodes / separate pieces of hardware
 

Briefly describe the three benchmarking procedures: LINPACK, HPCC, SHOC
LINPACK - Linear Algebra Package: Dense MAtrix Solver
HPCC - High-Performance Computing Challenge
Tests several things, including LINPACK, double precision matrix multiply, memory bandwidth, parallel matrix transpose, random memory updates, double precision complex discrete fourier transform, and communication bandwidth and latency
SHOC - Scalable Heterogeneous Computing
Non-traditional systems (GPU)
 

Briefly describe the three supercomputing rankings: TOP500, GREEN500, GRAPH500
TOP500 - Rank supercomputers based on LINPACK score
GREEN500 - Rank supercomputers with emphasis on energy usage (LINPACK / power consumption)
GRAPH500 - Rank systems based on benchmarks designed for data-intensive computing
 

Give the command to load the gcc compiler version 5.3.0 and the openmpi library version 10.3.1 into your environment
module add gcc/5.3.0 openmpi/10.3.1

 

What is a message tag?
an integer identifying the message. Programmer is responsible for managing tag

 

What is an MPI communicator? What is the purpose of the communicator?
Variable which:

Contains information related to number of processes and which process this code is being run from
Allows communication between other processes
 

In MPI, what is a process rank? What routine do you use to determine a process rank?
Process rank - unique ID that identifies a process

Get_rank()

 

True or false: In MPI you set the number of processes when you write the source code.
FALSE - determined dynamically

 

Explain the basic functionality of the collective calls Bcast, Scatter, Gather, Barrier, Reduce, and how they are the same or different from each other.
Bcast - sends same data to all other processes
Scatter - splits up data and sends unique data to other processes
Gather - same as scatter, but in reverse
Barrier - waits for all other processes to reach this point
Reduce - takes all data from each process and combines it, result is stored on root processor
 

With a short sentence, describe the role of MPI.COMM_WORLD
MPI.COMM_WORLD returns a variable which can be used to communicate with other processes and retrieve information about this process.

 

What is “Pleasantly Parallel?” List two characteristics of pleasantly parallel
“A computation that can obviously be divided into a number of completely different parts, each of which can be executed by a separate process.”

No communication or very little communication among the processes
Each process can do its tasks without any interaction with the other processes
 

Describe three workload assignment approaches for pleasantly parallel using a few sentences for each approach
Assume you have N processors to work on the task.

Static - Divide the workload into N equal chunks. Processor 0 takes the first chunk, processor 1 takes the second, chunk, ... , processor N-1 takes the N-1th chunk. Has large potential for bottleneck by giving a processor the heaviest workload.
Cyclic - Divide the workload into N*M equal chunks. Processor 0 takes the first chunk, the 1+Nth chunk, etc. Processor 1 takes the second chunk, the 2+Nth chunk, etc. Has less potential for bottleneck than approach 1.
Dynamic - Divide the workload into arbitrary number of chunks. Each processor requests the next available task whenever it is done with its current chunk, meaning each processor may not necessarily complete the same number of chunks. Requires more coordination between processes, but generally has the least amount of potential for bottleneck.



How do the worker processes know when to terminate in the dynamic workload assignment model?
They look for a specific tag sent with the message from the host process

 

With a short sentence each, describe the two basic partitioning approaches in parallel programming.
Data partitioning - also known as domain decomposition, partitions the data in a program.
Functional decomposition - partitions the functions of a program



A scientist wants to calculate the sum of 8,192 integers. The scientist has a 16-cores CPU that takes 0.1ns to transfer one integer from one core to another. Construct a tree that describes the divide-and-conquer process to calculate the sum. Calculate the communication time the “divide” period and the communication time for the “conquer” period.
 

DIVIDE:

1->2 - Sends 4096 ints left, 4096 ints right = 819.2ns

2->4 - Each sends 2048 ints left, 2048 ints right = 409.6ns

4->8 - Each sends 1024 ints left, 1024 ints right = 204.8ns

8->16 - Each sends 512 ints left, 512 ints right = 102.4ns

 

Total = 1536.0ns

 

CONQUER:

16->8 - 16 nodes send 1 int, 8 nodes receive 2 ints = 0.1ns

8->4 - 8 nodes send 1 int, 4 nodes receive 2 ints = 0.1ns

4->2 - 4 nodes send 1 int, 2 nodes receive 2 ints = 0.1ns

2->1 - 2 nodes send 1 int, 1 node receives 2 ints = 0.1ns

 

Total = 0.4ns





Given 8 processes ranked 0 through 7, sort the following array of integers in ascending order using the odd-even transposition sorting process. You should clearly indicate which integers belong to which process at every single stage of the sorting process. [13, 7, 14, 11, 10, 4, 15, 12, 3, 6, 16, 1, 2, 8, 9, 5]
0

1

2

3

4

5

6

7

Step 0

13, 7

14, 11

10, 4

15, 12

3, 6

16, 1

2, 8

9, 5

Step 0b

7, 13

11, 14

4, 10

12, 15

3, 6

1, 16

2, 8

5, 9

Step 1

7, 11

13, 4

14, 10

12, 3

15, 1

6, 2

16, 5

8, 9

Step 1b

7, 11

4, 13

10, 14

3, 12

1, 15

2, 6

5, 16

8, 9

Step 2

7,4

11,10

13,3

14,1

12,2

15,5

6,8

16,9

Step 2b

4,7

10,11

3,13

1,14

2,12

5,15

6,8

9,16

Step 3

4,7

10,3

11,1

13,2

14,5

12,6

15,8

9,16

Step 3b

4,7

3,10

1,11

2,13

5,14

6,12

8,15

9,16

Step 4

4,3

7,1

10,2

11,5

13,6

14,8

12,9

15,16

Step 4b

3,4

1,7

2,10

5,11

6,13

8,14

9,12

15,16

Step 5

3,1

4,2

7,5

10,6

11,8

13,9

14,12

15,16

Step 5b

1,3

2,4

5,7

6,10

8,11

9,13

12,14

15,16

Step 6

1,2

3,4

5,6

7,8

10,9

11,12

13,14

15,16

Step 6b

1,2

3,4

5,6

7,8

9,10

11,12

13,14

15,16

Be prepared to answer programming questions of the following type:
 

* Using only MPI_Send and MPI_Recv, implement MPI_Reduce.

 

# Implement this code using tree-structure, not sequential

# ie, rank 0 cannot just receive receive from ranks 1-N

 

# Works for even powers of 2

 

# Assumes the operation is add

value = rank + 1;

for i in range(int(math.log2(size)) - 1, -1, -1):

   distance = int(math.pow(2, i))

   if ( rank < distance ):

       add = comm.recv( source=rank + distance, tag=0);

       value = value + add;

   if (rank >= distance and rank < int(math.pow(2,i+1))):

       comm.send(value, dest=rank - distance, tag=0)

if rank == 0:

   print("Value = " + str(value))




* Using only MPI_Send and MPI_Recv, implement MPI_Scatter.

 

% Implement this code using tree-structure, not sequential

% ie, rank 0 cannot just receive receive from ranks 1-N





* Using only MPI_Send and MPI_Recv, implement MPI_Gather.

 

% Implement this code using tree-structure, not sequential

% ie, rank 0 cannot just receive receive from ranks 1-N

 

* Using only MPI_Send and MPI_Recv, implement MPI_Alltoall.

 

% Implement this code using tree-structure, not sequential

% ie, rank 0 cannot just receive receive from ranks 1-N




Suppose the following is run with 3 processes, with rank 0, 1, and 2, respectively. What are the values of x, y, and z in each process after the code has been executed?
 

comm = MPI.COMM_WORLD

if rank == 0:

 x=0; y=1; z=2;

 comm.Bcast(x, root = 0)

 comm.Send(y, des = 2, tag = 43)

 comm.Bcast(z, root = 1)

if rank == 1:  

 x=3, y=4, z=5;

 comm.Bcast(x, root = 0)

 comm.Bcast(y, root = 1)

if rank == 2:

 x=6; y=7; z=8;

 comm.Bcast(z, root = 0)

 comm.Recv(x, source = 0, tag = 43, status=status)

 comm.Bcast(y, root = 1)

 

Process 0 - Broadcasts 0 from 0, sends 1 to 2 with tag 43, broadcasts 2 to 1

Process 1 - Broadcasts 3 from 0, broadcasts y from 1

Process 2 - Broadcasts 8 from 0, receives from 0 with tag 43,




x

y

z

Process 0

0

1

4

Process 1

0

4

5

Process 2

1

4

0






Explain if the following MPI code segment is correct or not, and why:
Process 0 executes:

comm.Bcast(mydata, root = 0)

 

All other processes:

comm.Bcast(yourdata, root = 0)

 

SEemingly, since Bcast is uppercase, mydata and yourdata need to be in some kind of tuple/array.

 

Other than that, this seems to work.

 

Explain if the following MPI code segment is correct or not, and why:
Process 0 executes:

comm.Bcast(mydata, root = 0)

 

All other processes:

comm.Recv(mydata, source = 0, MPI.ANY_TAG, status = status)

 

This is incorrect, bcast() cannot be matched with a recv()




Explain if the following MPI code segment is correct or not, and why:
 

Process 0 executes:

comm.Recv(yourdata, source = 1, tag = 0, status = status)

comm.Send(mydata, des = 1, tag = 0)

 

Process 1 executes:

comm.Recv(yourdata, source = 0, tag = 0, status = status)

comm.Send(mydata, des = 0, tag = 0)

 

Will not work - both processes get blocked on first instance of Recv()

 

Give an example of a master/slave computation using MPI-like pseudocode.
Some example code-

http://www.mcs.anl.gov/research/projects/mpi/tutorial/mpiexmpl/src2/io/C/main.html

 

Understand and be able to read/trace through all codes discussed in class lectures.
 

Just do it. Codes repository

NOTE: He said in class to be able to understand the code for Transportation sort, Merge sort, and Quicksort, and to be able to tell the value(s) of the array(s) being sorted at any step of the code.

 

Understand the principles of the communication tree among the processes in divide-and-conquer problems.



What are the four main characteristics of Parallel File Systems?
Run on enterprise shared storage
The computing resource is often physically separate from the application
Usually large HPC applications that tend to perform lots I/O and have massive bandwidth requirements
Distribute data of a single object across multiple nodes
What are the two main differences between block-based and object-based parallel file systems?
Blocks are fixed width, objects are variable length
Block-based file systems require dynamic metadata for distribution info
Object-based file systems require static metadata for distribution info
Map Reduce

Distribute data for storage - hadoop
Parallelize data for consumption - map reduce
Handle failure - both
What is “map”? A function/procedure that is applied to every individual elements of a collection/list/array/…

int square(x) { return x*x;}
map square [1,2,3,4] -> [1,4,9,16]

What is “reduce”? A function/procedure that performs an operation on a list. This operation will “fold/reduce” this list into a single value (or a smaller subset)

reduce ([1,2,3,4]) using sum -> 10
reduce ([1,2,3,4]) using multiply -> 24

Hadoop Map Reduce:

Map function: Take in the input data and return a key,value pair
Reduce function: Receive the key,value pairs from the mapper and provide a final output as a reduction operation on the pairs
Optional functions:
Partition function: determines the distribution of mappers’ key,value pairs to the reducers
Combine functions: initial reduction on the mappers to reduce network traffics
Patterns:

Summarization - group similar data together and perform an action on it
Examples:
Numerical summarizations - calculate aggregate statistical values over a large data set
Inverted index - generate an index from data
Counters - use MapReduce’s internal counter to calculate sums on the entire data set
Filtering - don’t change actual records but find a subset of data in the entire set
Examples:
Filtering - evaluates each record and decides if it should stay or go
Top ten - retrieve small number of top records according to a ranking
Distinct - find a unique set of values, use deduplication processes
Data Organization - reorganizing data to help improve value, performance, usability
Examples:
Structured to Hierarchical - create new records with a different structure from the input data
Partitioning and Binning - use hadoops partitioner and binner to categorize data based on keys
Join - merge similar data from multiple data sets
Examples:
Reduce-side join - join by foreign keys
Map-side join - joining happens in map phase
Replicated join - (map side) use cache on mappers
Metapatterns - patterns that deal with patterns
Examples:
Join chaining - chaining several map reduce jobs
Job merging -
Word count example:

